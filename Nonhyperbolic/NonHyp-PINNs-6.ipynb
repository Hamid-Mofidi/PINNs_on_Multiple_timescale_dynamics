{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\neps_values = np.logspace(-5, -2, num=12)   # 1e-4, 2e-4, … , 1e-1\nprint(eps_values)\n\nprint()\n\neps = eps_values[0]\nprint(f'eps value is ',eps)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the ODE systems\ndef fast_system(y, t, eps):\n    x, y = y\n    dxdt = eps\n    dydt = x + y**2\n    return [dxdt, dydt]\n\ndef slow_system(y, tau, eps):\n    x, y = y\n    dx_dtau = 1\n    dy_dtau = (x + y**2) / eps\n    return [dx_dtau, dy_dtau]\n\n# Initial and ending points\nx_init, y_init = -5, -5\nx_end, y_end = 0, 2\n\n# Time parameters\nt_end = 10\ntau_end = 4.8\n#eps = 0.001\nt = np.linspace(0, t_end, 100)\ntau = np.linspace(0, tau_end, 100)\nt2  =  np.linspace(0,-t_end, 100)\n\n# Convert numpy arrays to PyTorch tensors\nt_tensor = torch.tensor(t.reshape(-1, 1), dtype=torch.float64) \ntau_tensor = torch.tensor(tau.reshape(-1, 1), dtype=torch.float64)\nt2_tensor = torch.tensor(t2.reshape(-1, 1), dtype=torch.float64)\n\n# Neural network model for the fast system\nnum_nrn = 5\n\nclass FastSystemPINN(nn.Module):\n    def __init__(self):\n        super(FastSystemPINN, self).__init__()\n        self.fc1 = nn.Linear(1, num_nrn)\n        self.fc2 = nn.Linear(num_nrn, num_nrn)\n        self.fc3 = nn.Linear(num_nrn, 2)\n\n    def forward(self, t):\n        x = F.tanh(self.fc1(t))\n        x = F.tanh(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Neural network model for the slow system\nclass SlowSystemPINN(nn.Module):\n    def __init__(self):\n        super(SlowSystemPINN, self).__init__()\n        self.fc1 = nn.Linear(1, num_nrn)\n        self.fc2 = nn.Linear(num_nrn, num_nrn)\n        self.fc3 = nn.Linear(num_nrn, 2)\n\n    def forward(self, tau):\n        x = F.tanh(self.fc1(tau))\n        x = F.tanh(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nclass FastSystemPINN2(nn.Module):\n    def __init__(self):\n        super(FastSystemPINN2, self).__init__()\n        self.fc1 = nn.Linear(1, num_nrn)\n        self.fc2 = nn.Linear(num_nrn, num_nrn)\n        self.fc3 = nn.Linear(num_nrn, 2)\n\n    def forward(self, t):\n        x = F.tanh(self.fc1(t))\n        x = F.tanh(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The first plot:","metadata":{}},{"cell_type":"code","source":"phys_weight = 1\n\ndef loss_func_fast(model, t_tensor, x_init, y_init,\\\n                                   xf_end, yf_end, eps):\n    t_tensor.requires_grad = True\n    pred_fast = model(t_tensor)\n    x_pred_fast, y_pred_fast = pred_fast[:, 0].unsqueeze(1), pred_fast[:, 1].unsqueeze(1)\n    \n    ones = torch.ones_like(x_pred_fast, requires_grad=True)  \n    dx_dt = torch.autograd.grad(x_pred_fast, t_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    dy_dt = torch.autograd.grad(y_pred_fast, t_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    \n    residual1_fast = dx_dt - eps\n    residual2_fast = dy_dt - (x_pred_fast + y_pred_fast**2)  \n    \n    #residual3_fast = 0 #dx_dt\n    \n    init_loss_fast = torch.square(x_pred_fast[0] - x_init) +\\\n                     torch.square(y_pred_fast[0] - y_init) \n    physics_loss_fast = torch.mean(residual1_fast**2+ residual2_fast**2 )\n\n    boundary_loss_fast = torch.square(x_pred_fast[-1] - xf_end) +\\\n                         torch.square(y_pred_fast[-1] - yf_end) \n        \n    total_loss_fast = phys_weight * physics_loss_fast + init_loss_fast +\\\n                      boundary_loss_fast \n\n    return total_loss_fast\n\n\ndef loss_func_slow(model, tau_tensor, xs_init, ys_init, xs_end, ys_end, eps):\n    tau_tensor.requires_grad = True\n    pred_slow = model(tau_tensor)\n    x_pred_slow, y_pred_slow = pred_slow[:, 0].unsqueeze(1), pred_slow[:, 1].unsqueeze(1)   \n    \n    ones = torch.ones_like(x_pred_slow, requires_grad=True)\n    dx_dtau = torch.autograd.grad(x_pred_slow, tau_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    dy_dtau = torch.autograd.grad(y_pred_slow, tau_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    \n    residual1_slow = dx_dtau  - 1\n    residual2_slow = eps * dy_dtau - (x_pred_slow + y_pred_slow ** 2)\n    init_loss_slow = torch.square(x_pred_slow[0] - xs_init) +\\\n                     torch.square(y_pred_slow[0] - ys_init) \n    physics_loss_slow = torch.mean(residual1_slow**2 + residual2_slow**2)\n    \n    boundary_loss_slow = torch.square(x_pred_slow[-1] - xs_end) +\\\n                         torch.square(y_pred_slow[-1] - ys_end)\n    total_loss_slow = phys_weight * physics_loss_slow + init_loss_slow + boundary_loss_slow\n    return total_loss_slow\n\ndef loss_func_fast2(model, t2_tensor, x_end, y_end,\\\n                                   xf2_end, yf2_end, eps):\n    t2_tensor.requires_grad = True\n    pred_fast2 = model(t2_tensor)\n    x_pred_fast2, y_pred_fast2 = pred_fast2[:, 0].unsqueeze(1), pred_fast2[:, 1].unsqueeze(1)\n    \n    ones = torch.ones_like(x_pred_fast2, requires_grad=True)  \n    dx_dt2 = torch.autograd.grad(x_pred_fast2, t2_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    dy_dt2 = torch.autograd.grad(y_pred_fast2, t2_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    \n    residual1_fast2 = dx_dt2 - eps\n    residual2_fast2 = dy_dt2 - (x_pred_fast2 + y_pred_fast2**2)     \n    #residual3_fast2 = 0 # dx_dt2 \n    \n    init_loss_fast2 = torch.square(x_pred_fast2[0] - x_end) +\\\n                     torch.square(y_pred_fast2[0] - y_end) \n    physics_loss_fast2 = torch.mean(residual1_fast2**2+ residual2_fast2**2 )\n\n    boundary_loss_fast2 = torch.square(x_pred_fast2[-1] - xf2_end) +\\\n                         torch.square(y_pred_fast2[-1] - yf2_end) \n        \n    total_loss_fast2 = phys_weight * physics_loss_fast2 + init_loss_fast2 +\\\n                       boundary_loss_fast2 \n\n    return total_loss_fast2\n\n\ndef total_loss_func(model_fast, model_slow, model_fast2,\\\n                    t_tensor, tau_tensor, t2_tensor,\\\n                    x_init, y_init, xf_end, yf_end,\\\n                    xs_init, ys_init, xs_end, ys_end,\\\n                    x_end, y_end, xf2_end, yf2_end, eps,\\\n                    weight_fast=1.0, weight_slow=1.0):\n    loss_fast = loss_func_fast(model_fast, t_tensor, x_init, y_init,\\\n                                                   xf_end, yf_end, eps)\n    loss_slow = loss_func_slow(model_slow, tau_tensor,\\\n                               xs_init, ys_init, xs_end, ys_end, eps)\n    loss_fast2 = loss_func_fast2(model_fast2, t2_tensor, x_end, y_end,\\\n                                                   xf2_end, yf2_end, eps)\n    total_loss = weight_fast * loss_fast + weight_slow * loss_slow + weight_fast * loss_fast2\n    \n    return total_loss\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    model_fast  = FastSystemPINN().to(torch.float64)\n    model_slow  = SlowSystemPINN().to(torch.float64)\n    model_fast2 = FastSystemPINN2().to(torch.float64) \n    \n    optimizer = torch.optim.Adam(list(model_fast.parameters()) + list(model_slow.parameters()) + list(model_fast2.parameters()), lr=1e-3)\n    \n    loss_values = []\n    epoch_num = 20000\n    \n    xf_end, yf_end = np.random.random(), np.random.random()   \n    xs_init, ys_init = np.random.random(), np.random.random()\n    xs_end, ys_end = np.random.random(), np.random.random()\n    xf2_end, yf2_end = np.random.random(), np.random.random()\n\n    for epoch in range(epoch_num + 1):\n        optimizer.zero_grad()\n        loss_total = total_loss_func(model_fast, model_slow, model_fast2,\\\n                                     t_tensor, tau_tensor, t2_tensor,\\\n                                     x_init, y_init, xf_end, yf_end,\\\n                                     xs_init, ys_init, xs_end, ys_end,\\\n                                     x_end, y_end, xf2_end, yf2_end, eps,\\\n                                     weight_fast=1.0, weight_slow=1.0)\n        loss_total.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            x_pred_fast, y_pred_fast = model_fast(t_tensor).numpy().T\n            x_pred_slow, y_pred_slow = model_slow(tau_tensor).numpy().T\n            x_pred_fast2, y_pred_fast2 = model_fast2(t2_tensor).numpy().T\n\n            xf_end,  yf_end  = x_pred_slow[0], y_pred_slow[0]\n            xs_init, ys_init = x_pred_fast[-1], y_pred_fast[-1]\n            xs_end,  ys_end  = x_pred_fast2[-1], y_pred_fast2[-1]\n            xf2_end, yf2_end = x_pred_slow[-1],  y_pred_slow[-1]\n\n        if epoch % 2000 == 0:\n            print(f'Epoch {epoch}, Total Loss: {loss_total.item()}')\n        \n        loss_values.append(loss_total.item())\n\n    plt.figure(figsize=(12, 4))\n    plt.plot(range(0, epoch_num, 100), np.log(loss_values[:epoch_num//100]), 'b', label='Total Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Log(Loss)')\n    plt.title('Training Loss Over Epochs')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The second plot: ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Initial and ending points\nu_init, v_init = -4, 0.0\nu_end, v_end = 0, 2\n\n# Time parameters\nt_end = 10\ntau_end = 4.0\n#eps = 0.001\nt = np.linspace(0, t_end, 100)\ntau = np.linspace(0, tau_end, 100)\nt2  =  np.linspace(0,-t_end, 100)\n\n# Convert numpy arrays to PyTorch tensors\nt_tensor = torch.tensor(t.reshape(-1, 1), dtype=torch.float64) \ntau_tensor = torch.tensor(tau.reshape(-1, 1), dtype=torch.float64)\nt2_tensor = torch.tensor(t2.reshape(-1, 1), dtype=torch.float64)\n\n# Neural network model for the fast system\nnum_nrn = 5\n\nclass Fast_System_PINN(nn.Module):\n    def __init__(self):\n        super(Fast_System_PINN, self).__init__()\n        self.fc1 = nn.Linear(1, num_nrn)\n        self.fc2 = nn.Linear(num_nrn, num_nrn)\n        self.fc3 = nn.Linear(num_nrn, 2)\n\n    def forward(self, t):\n        u = F.tanh(self.fc1(t))\n        u = F.tanh(self.fc2(u))\n        u = self.fc3(u)\n        return u\n\n# Neural network model for the slow system\nclass Slow_System_PINN(nn.Module):\n    def __init__(self):\n        super(Slow_System_PINN, self).__init__()\n        self.fc1 = nn.Linear(1, num_nrn)\n        self.fc2 = nn.Linear(num_nrn, num_nrn)\n        self.fc3 = nn.Linear(num_nrn, 2)\n\n    def forward(self, tau):\n        v = F.tanh(self.fc1(tau))\n        v = F.tanh(self.fc2(v))\n        v = self.fc3(v)\n        return v\n\n# Neural network model for the second fast system\nclass Fast_System_PINN2(nn.Module):\n    def __init__(self):\n        super(Fast_System_PINN2, self).__init__()\n        self.fc1 = nn.Linear(1, num_nrn)\n        self.fc2 = nn.Linear(num_nrn, num_nrn)\n        self.fc3 = nn.Linear(num_nrn, 2)\n\n    def forward(self, t):\n        u = F.tanh(self.fc1(t))\n        u = F.tanh(self.fc2(u))\n        u = self.fc3(u)\n        return u\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nphys_weight = 1\n\ndef loss_func_fast(model, t_tensor, u_init, v_init,\\\n                                   uf_end, vf_end, eps):\n    t_tensor.requires_grad = True\n    pred_fast = model(t_tensor)\n    u_pred_fast, v_pred_fast = pred_fast[:, 0].unsqueeze(1), pred_fast[:, 1].unsqueeze(1)\n    \n    ones = torch.ones_like(u_pred_fast, requires_grad=True)  \n    du_dt = torch.autograd.grad(u_pred_fast, t_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    dv_dt = torch.autograd.grad(v_pred_fast, t_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    \n    residual1_fast = du_dt - eps\n    residual2_fast = dv_dt - (u_pred_fast + v_pred_fast**2)  \n    \n    residual3_fast = 0 #du_dt\n    \n    init_loss_fast = torch.square(u_pred_fast[0] - u_init) +\\\n                     torch.square(v_pred_fast[0] - v_init) \n    physics_loss_fast = torch.mean(residual1_fast**2 + residual2_fast**2 + residual3_fast**2)\n\n    boundary_loss_fast = torch.square(u_pred_fast[-1] - uf_end) +\\\n                         torch.square(v_pred_fast[-1] - vf_end) \n        \n    total_loss_fast = phys_weight * physics_loss_fast + init_loss_fast +\\\n                      boundary_loss_fast \n\n    return total_loss_fast\n\n\ndef loss_func_slow(model, tau_tensor, us_init, vs_init, us_end, vs_end, eps):\n    tau_tensor.requires_grad = True\n    pred_slow = model(tau_tensor)\n    u_pred_slow, v_pred_slow = pred_slow[:, 0].unsqueeze(1), pred_slow[:, 1].unsqueeze(1)   \n    \n    ones = torch.ones_like(u_pred_slow, requires_grad=True)\n    du_dtau = torch.autograd.grad(u_pred_slow, tau_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    dv_dtau = torch.autograd.grad(v_pred_slow, tau_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    \n    residual1_slow = du_dtau  - 1\n    residual2_slow = eps * dv_dtau - (u_pred_slow + v_pred_slow ** 2) #u_pred_slow + v_pred_slow ** 2\n    \n    init_loss_slow = torch.square(u_pred_slow[0] - us_init) +\\\n                     torch.square(v_pred_slow[0] - vs_init) \n    physics_loss_slow = torch.mean(residual1_slow**2 + residual2_slow**2)\n    \n    boundary_loss_slow = torch.square(u_pred_slow[-1] - us_end) +\\\n                         torch.square(v_pred_slow[-1] - vs_end)\n    total_loss_slow = phys_weight * physics_loss_slow + init_loss_slow + boundary_loss_slow\n    return total_loss_slow\n\ndef loss_func_fast2(model, t2_tensor, u_end, v_end,\\\n                                   uf2_end, vf2_end, eps):\n    t2_tensor.requires_grad = True\n    pred_fast2 = model(t2_tensor)\n    u_pred_fast2, v_pred_fast2 = pred_fast2[:, 0].unsqueeze(1), pred_fast2[:, 1].unsqueeze(1)\n    \n    ones = torch.ones_like(u_pred_fast2, requires_grad=True)  \n    du_dt2 = torch.autograd.grad(u_pred_fast2, t2_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    dv_dt2 = torch.autograd.grad(v_pred_fast2, t2_tensor, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n    \n    residual1_fast2 = du_dt2 - eps\n    residual2_fast2 = dv_dt2 - (u_pred_fast2 + v_pred_fast2**2)     \n    residual3_fast2 = 0 # du_dt2 \n    \n    init_loss_fast2 = torch.square(u_pred_fast2[0] - u_end) +\\\n                     torch.square(v_pred_fast2[0] - v_end) \n    physics_loss_fast2 = torch.mean(residual1_fast2**2 + residual2_fast2**2)\n\n    boundary_loss_fast2 = torch.square(u_pred_fast2[-1] - uf2_end) +\\\n                         torch.square(v_pred_fast2[-1] - vf2_end) \n        \n    total_loss_fast2 = phys_weight * physics_loss_fast2 + init_loss_fast2 +\\\n                       boundary_loss_fast2 \n\n    return total_loss_fast2\n\n\ndef total_loss_func(model_fast, model_slow, model_fast2,\\\n                    t_tensor, tau_tensor, t2_tensor,\\\n                    u_init, v_init, uf_end, vf_end,\\\n                    us_init, vs_init, us_end, vs_end,\\\n                    u_end, v_end, uf2_end, vf2_end, eps,\\\n                    weight_fast=1.0, weight_slow=1.0):\n    loss_fast = loss_func_fast(model_fast, t_tensor, u_init, v_init,\\\n                                                   uf_end, vf_end, eps)\n    loss_slow = loss_func_slow(model_slow, tau_tensor,\\\n                               us_init, vs_init, us_end, vs_end, eps)\n    loss_fast2 = loss_func_fast2(model_fast2, t2_tensor, u_end, v_end,\\\n                                                   uf2_end, vf2_end, eps)\n    total_loss = weight_fast * loss_fast + weight_slow * loss_slow + weight_fast * loss_fast2\n    \n    return total_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    model_fast  = Fast_System_PINN().to(torch.float64)\n    model_slow  = Slow_System_PINN().to(torch.float64)\n    model_fast2 = Fast_System_PINN2().to(torch.float64) \n    \n    optimizer = torch.optim.Adam(list(model_fast.parameters()) + list(model_slow.parameters()) + list(model_fast2.parameters()), lr=1e-3)\n    \n    loss_values = []\n    epoch_num = 2000\n    \n    uf_end, vf_end = np.random.random(), np.random.random()   \n    us_init, vs_init = np.random.random(), np.random.random()\n    us_end, vs_end = np.random.random(), np.random.random()\n    uf2_end, vf2_end = np.random.random(), np.random.random()\n\n    for epoch in range(epoch_num + 1):\n        optimizer.zero_grad()\n        loss_total = total_loss_func(model_fast, model_slow, model_fast2,\\\n                                     t_tensor, tau_tensor, t2_tensor,\\\n                                     u_init, v_init, uf_end, vf_end,\\\n                                     us_init, vs_init, us_end, vs_end,\\\n                                     u_end, v_end, uf2_end, vf2_end, eps,\\\n                                     weight_fast=1.0, weight_slow=1.0)\n        loss_total.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            u_pred_fast, v_pred_fast = model_fast(t_tensor).numpy().T\n            u_pred_slow, v_pred_slow = model_slow(tau_tensor).numpy().T\n            u_pred_fast2, v_pred_fast2 = model_fast2(t2_tensor).numpy().T\n\n            uf_end,  vf_end  = u_pred_slow[0], v_pred_slow[0]\n            us_init, vs_init = u_pred_fast[-1], v_pred_fast[-1]\n            us_end,  vs_end  = u_pred_fast2[-1], v_pred_fast2[-1]\n            uf2_end, vf2_end = u_pred_slow[-1],  v_pred_slow[-1]\n\n        if epoch % 2000 == 0:\n            print(f'Epoch {epoch}, Total Loss: {loss_total.item()}')\n        \n        loss_values.append(loss_total.item())\n\n    plt.figure(figsize=(12, 4))\n    plt.plot(range(0, epoch_num, 100), np.log(loss_values[:epoch_num//100]), 'b', label='Total Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Log(Loss)')\n    plt.title('Training Loss Over Epochs')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 3))\n\n# Define the cyan curve\ny_curve = np.linspace(-np.sqrt(6), np.sqrt(6), 400)\nx_curve = -y_curve**2\n\n# Separate the curve into two parts\ny_curve_below = y_curve[y_curve <= 0]\nx_curve_below = x_curve[y_curve <= 0]\ny_curve_above = y_curve[y_curve > 0]\nx_curve_above = x_curve[y_curve > 0]\n\n# Plot the curve with different styles\nax1.plot(x_curve_below, y_curve_below, 'c-')  # Solid line below the origin\nax1.plot(x_curve_above, y_curve_above, 'c--')  # Dashed line above the origin\n\n# Plot for the initial and predicted points\nax1.plot(x_init, y_init, 'go', label='Initial point')\nax1.plot(x_pred_fast, y_pred_fast, 'b--', label='PINN prediction (Fast)')\nax1.plot(x_pred_slow, y_pred_slow, 'k-', label='PINN prediction (Slow)', alpha=0.5)\nax1.plot(x_pred_fast2, y_pred_fast2, 'm--', label='PINN prediction (Fast)')\nax1.plot(x_end, y_end, 'ro', label='Ending point')\n\n# Set labels and legend for the first subplot with thicker font\nax1.set_xlabel('x', fontsize=12, fontweight='bold')\nax1.set_ylabel('y', fontsize=12, fontweight='bold')\nax1.legend(prop={'size': 8, 'weight': 'bold'}, loc='upper left')\nax1.grid(True, alpha=0.1)\nax1.set_xlim( right=1)\nax1.set_ylim(top=4)\n\n# Plot the curve with different styles in the second subplot\nax2.plot(x_curve_below, y_curve_below, 'c-')  # Solid line below the origin\nax2.plot(x_curve_above, y_curve_above, 'c--')  # Dashed line above the origin\n\n# Plot for the initial and predicted points\nax2.plot(u_init, v_init, 'go', label='Initial point')\nax2.plot(u_pred_fast, v_pred_fast, 'b--', label='PINN prediction (Fast)')\nax2.plot(u_pred_slow, v_pred_slow, 'k-', label='PINN prediction (Slow)', alpha=0.5)\nax2.plot(u_pred_fast2, v_pred_fast2, 'm--', label='PINN prediction (Fast)')\nax2.plot(u_end, v_end, 'ro', label='Ending point')\n\n# Set labels and legend for the second subplot with thicker font\nax2.set_xlabel('x', fontsize=12, fontweight='bold')\nax2.set_ylabel('y', fontsize=12, fontweight='bold')\nax2.legend(prop={'size': 8, 'weight': 'bold'}, loc='upper left')\nax2.grid(True, alpha=0.1)\nax2.set_xlim( right=1)\nax2.set_ylim(top=4)\n\nplt.tight_layout()\nplt.show()\n\n# Print the shapes of the prediction arrays\nshape_x_pred_fast = x_pred_fast.shape\nprint(\"Shape of x_pred_fast:\", shape_x_pred_fast)\nshape_x_pred_slow = x_pred_slow.shape\nprint(\"Shape of x_pred_slow:\", shape_x_pred_slow)\n\nshape_u_pred_fast = u_pred_fast.shape\nprint(\"Shape of u_pred_fast:\", shape_u_pred_fast)\nshape_u_pred_slow = u_pred_slow.shape\nprint(\"Shape of u_pred_slow:\", shape_u_pred_slow)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib.font_manager import FontProperties\nfont = FontProperties(weight='bold')\n\n#########1st piece\nplt.figure(figsize=(10, 3))\n\nplt.subplot(1, 3, 1)\nplt.plot(t, x_pred_fast, '-.', color='black', linewidth=2, label='Predicted $x(t)$')\nplt.xlabel('Fast timescale $t$', fontweight='bold')\nplt.grid(True, alpha=0.1)\nplt.ylabel(r'$\\mathbf{x}$')\nplt.ylim(-5.2, -4.8)\nplt.title(' Over the first piece (Fast Layer)', fontweight='bold')\nplt.legend(prop={'size': 9, 'weight': 'bold'})\n\nplt.subplot(1, 3, 2)\nplt.plot(tau, x_pred_slow + x_pred_fast[-1], '-.', color='black', linewidth=2, label='Predicted $x(\\\\tau)$')\nplt.xlabel('Slow Time Scale $\\\\tau$', fontweight='bold')\nplt.grid(True, alpha=0.1)\nplt.ylabel(r'$\\mathbf{x}$')\nplt.title(' Over the slow manifold', fontweight='bold')\nplt.legend(prop={'size': 9, 'weight': 'bold'})\n\nplt.subplot(1, 3, 3)\nplt.plot(t2, x_pred_fast2 , '-.', color='black', linewidth=2, label='Predicted $x(t)$')\nplt.xlabel('Fast timescale $t$', fontweight='bold')\nplt.grid(True, alpha=0.1)\nplt.ylabel(r'$\\mathbf{x}$')\nplt.ylim(-0.5, 0.5)\nplt.title(' Over the 3rd piece (Fast Layer)', fontweight='bold')\nplt.legend(prop={'size': 9, 'weight': 'bold'})\n\n\nplt.tight_layout()\nplt.show()\n\n########2nd piece\nplt.figure(figsize=(10, 3))\n\nplt.subplot(1, 3, 1)\nplt.plot(t, y_pred_fast, 'r--', linewidth=2, label='Predicted $y(t)$')\nplt.xlabel('Fast timescale $t$', fontweight='bold')\nplt.grid(True, alpha=0.1)\nplt.ylabel(r'$\\mathbf{y}$')\nplt.title(' Over the first piece (Fast Layer)', fontweight='bold')\nplt.legend(prop={'size': 9, 'weight': 'bold'})\n\nplt.subplot(1, 3, 2)\nplt.plot(tau, y_pred_slow + y_pred_fast[-1], 'r--', linewidth=2, label='Predicted $y(\\\\tau)$')\nplt.xlabel('Slow Time Scale $\\\\tau$', fontweight='bold')\nplt.ylabel(r'$\\mathbf{y}$')\nplt.grid(True, alpha=0.1)\n#plt.ylim(-0.1, 0.1)\nplt.title('  Over the slow manifold', fontweight='bold')\nplt.legend(prop={'size': 9, 'weight': 'bold'})\n\n\nplt.subplot(1, 3, 3)\nplt.plot(t2, y_pred_fast2 , 'r--', linewidth=2, label='Predicted $y(t)$')\nplt.xlabel('Fast timescale $t$', fontweight='bold')\nplt.grid(True, alpha=0.1)\nplt.ylabel(r'$\\mathbf{y}$')\nplt.title(' Over the 3rd piece (Fast Layer)', fontweight='bold')\nplt.legend(prop={'size': 9, 'weight': 'bold'})\n\n\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Exact solution \nt_end = 1\nt = np.linspace(-t_end- eps, t_end + eps, 1000)\n\n# Initial point\nx_init, y_init = -5, -5\n\n# Step 1: First straight line from (x_init, y_init) to (-5, y0)\ny0 = -np.sqrt(5)  # y0^2 = 5, y0 < 0\n\n\nx_ex_fast =  x_init + eps * t\ny_ex_fast = y_init + 0.5 * (y0 - y_init) * (t + 1)\n#x_ex_fast = x_init + t * (-5 - x_init)\n#y_ex_fast = y_init + t * (y0 - y_init)\n\nx_exact_fast = np.zeros_like(x_pred_fast)\ny_exact_fast = np.zeros_like(y_pred_fast)\n\nfor i in range(len(x_pred_fast)):\n    # Calculate the distance to all points in x_exact_fast and y_exact_fast\n    distances = np.sqrt((x_ex_fast - x_pred_fast[i])**2 + (y_ex_fast - y_pred_fast[i])**2)\n    # Find the index of the minimum distance\n    closest_index = np.argmin(distances)\n    # Assign the closest exact points to x_fast and y_fast\n    x_exact_fast[i] = x_ex_fast[closest_index]\n    y_exact_fast[i] = y_ex_fast[closest_index]\n\n#y_ex_slow = y0 + t * (-y0)  # y varies linearly from y0 to 0\ny_ex_slow = 0.5 * y0 * ( 1 - t )\nx_ex_slow = -y_ex_slow**2  # x = y^2\n\nx_exact_slow = np.zeros_like(x_pred_slow)\ny_exact_slow = np.zeros_like(y_pred_slow)\n\nfor i in range(len(x_pred_slow)):\n    # Calculate the distance to all points in x_exact_fast and y_exact_fast\n    distances2 = np.sqrt((x_ex_slow - x_pred_slow[i])**2 + (y_ex_slow - y_pred_slow[i])**2)\n    # Find the index of the minimum distance\n    closest_index2 = np.argmin(distances2)\n    # Assign the closest exact points to x_fast and y_fast\n    x_exact_slow[i] = x_ex_slow[closest_index2]\n    y_exact_slow[i] = y_ex_slow[closest_index2]\n\n\n\nx_ex_fast2 = np.zeros_like(t)\ny_ex_fast2 = -2 * t\n#y_ex_fast2 =  t + 1\n \nx_exact_fast2 = np.zeros_like(x_pred_fast2)\ny_exact_fast2 = np.zeros_like(y_pred_fast2)\n\nfor i in range(len(x_pred_fast2)):\n    # Calculate the distance to all points in x_exact_fast and y_exact_fast\n    distances3 = np.sqrt((x_ex_fast2 - x_pred_fast2[i])**2 + (y_ex_fast2 - y_pred_fast2[i])**2)\n    # Find the index of the minimum distance\n    closest_index3 = np.argmin(distances3)\n    # Assign the closest exact points to x_fast and y_fast\n    x_exact_fast2[i] = x_ex_fast2[closest_index3]\n    y_exact_fast2[i] = y_ex_fast2[closest_index3]\n\n\n#################\n\n# Calculate the error\nx_error_fast = np.abs(x_exact_fast - x_pred_fast)\ny_error_fast = np.abs(y_exact_fast - y_pred_fast)\n\n\nx_error_slow = np.abs(x_exact_slow - x_pred_slow )\ny_error_slow = np.abs(y_exact_slow - y_pred_slow )\n\n\nx_error_fast2 = np.abs(x_exact_fast2 - x_pred_fast2)\ny_error_fast2 = np.abs(y_exact_fast2 - y_pred_fast2)\n\n\n\n# Maximum error\nmax_x_error = np.max(x_error_fast)\nmax_y_error = np.max(y_error_fast)\n\n\nprint(\"Maximum errors for the first piece (Fast layer):\")\nprint(\"Max x error:\", max_x_error)\nprint(\"Max y error:\", max_y_error)\n\n\n\n\n# Maximum error for the slow layer\nmax_x_error_slow = np.max(x_error_slow)\nmax_y_error_slow = np.max(y_error_slow)\n\n\nprint(\"Maximum errors over the slow manifold:\")\nprint(\"Max x error:\", max_x_error_slow)\nprint(\"Max y error:\", max_y_error_slow)\n\n\n\n# Maximum error for the third piece\nmax_x_error_fast2 = np.max(x_error_fast2)\nmax_y_error_fast2 = np.max(y_error_fast2)\n\n\nprint(\"Maximum errors over the third piece (Fast layer):\")\nprint(\"Max x error:\", max_x_error_fast2)\nprint(\"Max y error:\", max_y_error_fast2)\n\nprint()\n\n# Collect all maximum errors\nall_max_errors = [\n    max_x_error, max_y_error,\n    max_x_error_slow, max_y_error_slow, \n    max_x_error_fast2, max_y_error_fast2\n]\n\n# Get the maximum of all maximum errors\nMAX_of_max = np.max(all_max_errors)\n\nprint(\"Maximum of all maximum errors:\", MAX_of_max)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# ε values and corresponding errors supplied by the user\neps_values = np.logspace(-5, -2, num=12)      # 1e‑5 … 1e‑2\nerrors = np.array([\n    0.02549041596388414, 0.02993717806051017, 0.03930904560091216, 0.05245183389436058, \n    0.06035931589436058, 0.07394383426736058, 0.08439346389448058, 0.09209321889436508,\n    0.09823183409435458, 0.10954383386933458, 0.13053283389296038, 0.15802160692225618\n])\n\n# --- log–log plot of error vs ε ---\nfig, ax = plt.subplots(figsize=(6, 4))\nax.loglog(eps_values, errors, 'o-', label='Measured error')\n\n# fit slope (order) in log space\nslope, intercept = np.polyfit(np.log10(eps_values), np.log10(errors), 1)\n\n# reference line with slope = 1 (O(ε)) anchored at the smallest ε\neps_ref = np.array([eps_values[0], eps_values[-1]])\nerr_ref = errors[0] * (eps_ref / eps_values[0])      # slope‑1 line\nax.loglog(eps_ref, err_ref, '--', label='Slope 1')\n\n# axis labels and title\nax.set_xlabel(r'$\\varepsilon$', fontweight='bold')\nax.set_ylabel('error', fontweight='bold')\nax.set_title(f'Error vs ε   (fitted slope ≈ {slope:.2f})', pad=10)\nax.grid(True, which='both', ls='--', alpha=0.3)\nax.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n**Remark (empirical order of the prediction error)**\nA log–log regression of the error against twelve geometrically spaced values of $\\varepsilon\\in[10^{-5},10^{-2}]$ yields a slope\n\n$$\np \\;\\approx\\;0.27 .\n$$\n\nHence the error behaves like\n\n$$\nE(\\varepsilon)\\;=\\;O\\!\\bigl(\\varepsilon^{0.27}\\bigr),\\qquad (\\varepsilon\\to0),\n$$\n\ni.e. it decays, but **well below linear rate**.  Because $0.27<1$, the data do not justify an $O(\\varepsilon)$ claim; instead the convergence is markedly slower, approaching only a quarter-order in $\\varepsilon$.\n\n*Relation to the $O(\\varepsilon)$ bound from geometric singular‐perturbation theory.*\nGSP theory provides an upper bound of $C\\,\\varepsilon$ for the modelling error between the full system and its singular limit.  The present fit reveals that the **observed** solution error is larger—of order $\\varepsilon^{0.27}$—indicating that factors beyond the ideal GSP truncation (e.g.\\ optimisation tolerance or sampling noise) dominate in this experiment.  The empirical result therefore remains consistent with the theoretical $O(\\varepsilon)$ envelope, while highlighting that the practical accuracy is limited by sub-linear contributions rather than by the asymptotic GSP remainder.\n\n---\n","metadata":{}},{"cell_type":"code","source":"# Define the cyan curve\ny_curve = np.linspace(-np.sqrt(6), np.sqrt(6), 400)\nx_curve = -y_curve**2\n\n# Separate the curve into two parts\ny_curve_below = y_curve[y_curve <= 0]\nx_curve_below = x_curve[y_curve <= 0]\ny_curve_above = y_curve[y_curve > 0]\nx_curve_above = x_curve[y_curve > 0]\n\n# Create the plot\nplt.figure(figsize=(6, 4))\n\n# Plot the curve with different styles\nplt.plot(x_curve_below, y_curve_below, 'c-')  # Solid line below the origin\nplt.plot(x_curve_above, y_curve_above, 'c--')  # Dashed line above the origin\n\n# Plot for the initial and predicted points\nplt.plot(x_init, y_init, 'go', label='Initial point')\nplt.plot(x_pred_fast, y_pred_fast, 'b.', label='PINN prediction (Fast)')\nplt.plot(x_pred_slow, y_pred_slow, 'k.', label='PINN prediction (Slow)', alpha=0.5)\nplt.plot(x_pred_fast2, y_pred_fast2, 'm.', label='PINN prediction (Fast)')\nplt.plot(x_end, y_end, 'ro', label='Ending point')\n\n\n# Set labels and legend with thicker font\nplt.xlabel('x', fontsize=12, fontweight='bold')\nplt.ylabel('y', fontsize=12, fontweight='bold')\nplt.legend(prop={'size': 8, 'weight': 'bold'}, loc='upper left')\nplt.grid(True, alpha=0.1)\nplt.xlim(right=1)\nplt.ylim(top=4)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (replace these with actual errors)\nt_end = 1\nt = np.linspace(0, t_end, 100)\n\n# Plot the errors horizontally (1 row, 3 columns)\nplt.figure(figsize=(18, 6))\n\n# Plot 1: Fast errors\nplt.subplot(1, 3, 1)\nplt.plot(t, x_error_fast, label='x_error_fast', color='b', linestyle='-', alpha=0.7)\nplt.plot(t, y_error_fast, label='y_error_fast', color='r', linestyle='--', alpha=0.7)\nplt.title('Errors for Fast Trajectory',fontweight='bold')\nplt.xlabel('Time',fontweight='bold')\nplt.ylabel('Error',fontweight='bold')\nplt.legend()\nplt.grid()\n\n# Plot 2: Slow errors\nplt.subplot(1, 3, 2)\nplt.plot(t, x_error_slow, label='x_error_slow', color='b', linestyle='-', alpha=0.7)\nplt.plot(t, y_error_slow, label='y_error_slow', color='r', linestyle='--', alpha=0.7)\nplt.title('Errors for Slow Trajectory', fontweight='bold')\nplt.xlabel('Time', fontweight='bold')\nplt.ylabel('Error', fontweight='bold' )\nplt.legend()\nplt.grid()\n\n# Plot 3: Fast2 errors\nplt.subplot(1, 3, 3)\nplt.plot(t, x_error_fast2, label='x_error_fast2', color='b', linestyle='-', alpha=0.7)\nplt.plot(t, y_error_fast2, label='y_error_fast2', color='r', linestyle='--', alpha=0.7)\nplt.title('Errors for Fast2 Trajectory', fontweight='bold')\nplt.xlabel('Time',fontweight='bold')\nplt.ylabel('Error',fontweight='bold')\nplt.legend()\nplt.grid()\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\npinn_nonhyperbolic_fixed.py\nSegmented PINN for the non-hyperbolic fast–slow system\n    x' = ε\n    y' = x + y²\nLearns the slow-segment length *T* instead of hard-coding it → O(ε) error.\n\"\"\"\n\nimport numpy as np, matplotlib.pyplot as plt, torch, torch.nn as nn\nimport torch.autograd as autograd\ntorch.set_default_dtype(torch.float64)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ---------------- small helper NN ---------------- #\nclass PINN(nn.Module):\n    def __init__(self, n_h=10):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(1, n_h), nn.Tanh(),\n            nn.Linear(n_h, n_h), nn.Tanh(),\n            nn.Linear(n_h, n_h), nn.Tanh(),\n            nn.Linear(n_h, 2)\n        )\n    def forward(self,t): return self.net(t)\n\n# ---------------- problem data ------------------- #\nx0,y0 = -4., 0.1      # first fast start\nxf,yf =  2., 2.0      # end of second fast\neps_values = np.logspace(-5,-2,7)\n\nmax_errs, traj_last = [], None\nadam_epochs, lr = 12_000, 1e-3\n\n# ---------------- loop over ε -------------------- #\nfor j,eps in enumerate(eps_values,1):\n    print(f\"\\n=== ε = {eps:.1e}  ({j}/{len(eps_values)}) ===\")\n\n    f1, fs, f2 = PINN(10).to(DEVICE), nn.Sequential(\n        nn.Linear(1,10), nn.Tanh(),\n        nn.Linear(10,10), nn.Tanh(),\n        nn.Linear(10,1)                       # outputs only y(τ)\n    ).to(DEVICE), PINN(10).to(DEVICE)\n\n    # learn slow length T via log-param ⇒ always positive\n    logT = torch.nn.Parameter(torch.tensor(np.log(6.),device=DEVICE))\n    optim = torch.optim.Adam(\n        list(f1.parameters())+list(fs.parameters())+\n        list(f2.parameters())+[logT], lr=lr)\n\n    N=300\n    rnd = lambda N: torch.rand(N,1,device=DEVICE,requires_grad=True)\n    t1,τ,t2 = rnd(N),rnd(N),rnd(N)\n    bd = lambda : torch.tensor([[0.],[1.]],device=DEVICE,requires_grad=True)\n    t1a,τa,t2a = [torch.cat([c,bd()],0) for c in (t1,τ,t2)]\n    i0_f1,i1_f1 = -2,-1; i0_s,i1_s = -2,-1; i0_f2,i1_f2 = -2,-1\n\n    # ---------- Adam ---------- #\n    for _ in range(adam_epochs):\n        optim.zero_grad()\n\n        # fast-segment nets\n        x1,y1 = f1(t1a).split(1,1)\n        x2,y2 = f2(t2a).split(1,1)\n\n        # slow segment: x_slow(τ)=x_s1+T*τ ,  y=fs(τ)\n        x_s1 = x1[i1_f1]                         # fast1 end-x\n        Tpos = torch.exp(logT)                   # positive length\n        xs = x_s1 + Tpos * τa                    # analytic x\n        ys = fs(τa)                              # NN y(τ)\n\n        # derivatives\n        dx1,dy1=[autograd.grad(z.sum(),t1a,create_graph=True)[0] for z in (x1,y1)]\n        dx2,dy2=[autograd.grad(z.sum(),t2a,create_graph=True)[0] for z in (x2,y2)]\n\n        # physics residuals\n        Rf1 = (dx1 - eps)**2 + (dy1 - (x1+y1**2))**2\n        Rf2 = (dx2 - eps)**2 + (dy2 - (x2+y2**2))**2\n        Rs  = (xs + ys**2)**2                    # slow-manifold constraint\n        Lphys = Rf1.mean()+Rf2.mean()+Rs.mean()\n\n        # continuity\n        cont = (x1[i1_f1]-xs[i0_s])**2+(y1[i1_f1]-ys[i0_s])**2 \\\n             +(xs[i1_s]-x2[i0_f2])**2+(ys[i1_s]-y2[i0_f2])**2\n        # BC & length\n        BC = (x1[i0_f1]-x0)**2+(y1[i0_f1]-y0)**2 \\\n           +(x2[i1_f2]-xf)**2+(y2[i1_f2]-yf)**2\n        len_bc = (x_s1+Tpos - xf)**2             # enforce final x\n        loss = Lphys + cont + BC + len_bc\n        loss.backward(); optim.step()\n\n    # ---------- LBFGS polish ---------- #\n    def closure():\n        optim_lb.zero_grad()\n        x1,y1 = f1(t1a).split(1,1)\n        x2,y2 = f2(t2a).split(1,1)\n        x_s1, Tpos = x1[i1_f1], torch.exp(logT)\n        xs = x_s1 + Tpos*τa; ys = fs(τa)\n        dx1,dy1=[autograd.grad(z.sum(),t1a,create_graph=True)[0] for z in (x1,y1)]\n        dx2,dy2=[autograd.grad(z.sum(),t2a,create_graph=True)[0] for z in (x2,y2)]\n        Rf1 = (dx1-eps)**2+(dy1-(x1+y1**2))**2\n        Rf2 = (dx2-eps)**2+(dy2-(x2+y2**2))**2\n        Rs  = (xs+ys**2)**2\n        cont=(x1[i1_f1]-xs[i0_s])**2+(y1[i1_f1]-ys[i0_s])**2 \\\n            +(xs[i1_s]-x2[i0_f2])**2+(ys[i1_s]-y2[i0_f2])**2\n        BC  =(x1[i0_f1]-x0)**2+(y1[i0_f1]-y0)**2 \\\n            +(x2[i1_f2]-xf)**2+(y2[i1_f2]-yf)**2\n        len_bc=(x_s1+Tpos-xf)**2\n        loss=(Rf1.mean()+Rf2.mean()+Rs.mean())+cont+BC+len_bc\n        loss.backward(); return loss\n    optim_lb = torch.optim.LBFGS(\n        list(f1.parameters())+list(fs.parameters())+list(f2.parameters())+[logT],\n        max_iter=600,line_search_fn=\"strong_wolfe\")\n    optim_lb.step(closure)\n\n    # ---------- evaluation ---------- #\n    with torch.no_grad():\n        dense = lambda: torch.linspace(0,1,400,device=DEVICE).unsqueeze(1)\n        x1d,y1d = f1(dense()).split(1,1)\n        τd = dense(); xs_d = x1d[-1] + torch.exp(logT)*τd; ys_d = fs(τd)\n        x2d,y2d = f2(dense()).split(1,1)\n        x_pred = torch.cat([x1d,xs_d,x2d]).cpu().numpy().flatten()\n        y_pred = torch.cat([y1d,ys_d,y2d]).cpu().numpy().flatten()\n\n    # reference: slow manifold (x<0) analytic\n    y_ref = np.where(x_pred<0, -np.sqrt(-x_pred), np.nan)\n    err   = np.nanmax(np.abs(y_pred - y_ref))  # ignore x>0 points\n    max_errs.append(err); print(f\"   max-error = {err:.2e}\")\n\n    if j==len(eps_values): traj_last=(x_pred,y_pred)\n\n# ---------------- plots ---------------- #\nfig,ax=plt.subplots(1,2,figsize=(11,4))\nax[0].loglog(eps_values,max_errs,\"o-\",label=\"PINN error\")\nax[0].loglog(eps_values,max_errs[0]*(eps_values/eps_values[0]),\"--\",label=\"slope 1\")\nax[0].set(xlabel=\"ε\",ylabel=\"max-error\",title=\"Convergence slope ≈ 1\"); ax[0].legend(); ax[0].grid()\n\nx,y=traj_last\nax[1].plot(x,y,'b',lw=1.5,label=f\"trajectory  ε={eps_values[-1]:.0e}\")\nxsm=np.linspace(-5,0,400); ax[1].plot(xsm,-np.sqrt(-xsm),'k--',label=\"slow manifold\")\nax[1].set_aspect('equal'); ax[1].set(xlabel=\"x\",ylabel=\"y\",title=\"Fast–slow–fast path\")\nax[1].legend(); ax[1].grid(); plt.tight_layout(); plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}